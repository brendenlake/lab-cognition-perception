
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Estimating population parameters &#8212; Lab in C&amp;P (Fall 2021)</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/nyustyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/artificialintelligence.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Lab in C&P (Fall 2021)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../course-content/syllabus.html">
   Syllabus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../course-content/schedule.html">
   Schedule
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://edstem.org/us/courses/8295/discussion/">
   EdStem
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Textbook
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00/00-cogsci.html">
   1. What is Cognitive Science and how do we study it?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../01/00-whystats.html">
   2. Why do we have to learn statistics?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/00-jupyter.html">
   3. Introduction to Jupyter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03/00-python.html">
   4. Intro to Python for Psychology Undergrads
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/00-researchdesign.html">
   5. A brief introduction to research design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/00-data.html">
   6. The Format and Structure of Digital Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06/00-plots.html">
   7. Visualizing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07/00-describingdata.html">
   8. Describing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01-sampling.html">
   9. Samples, populations and sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/00-hypothesistesting.html">
   10. Hypothesis testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10/00-ttest.html">
   11. Comparing one or two means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../11/00-inferences-from-behavior.html">
   12. Measuring Behavior
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../21/00-ethics-irb.html">
   13. Research Ethics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../13/00-linearregression.html">
   14. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../14/00-logisticregression.html">
   15. Logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../15/00-mixed-effect.html">
   16. Linear Mixed Effect Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../16/00-mentalsimulation.html">
   17. Mental Imagery, Mental Simulation, and Mental Rotation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../17/00-mri.html">
   18. Magnetic Resonance Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../24/00-what-next.html">
   19. What Next?
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Labs &amp; Homeworks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00/cogsci-ica.html">
   Intro to CogSci ICA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../homeworks/Homework1.html">
   Intro to Jupyter (HW1)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tips &amp; Tricks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/pythonresources.html">
   Python Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/plottingresources.html">
   Plotting in Python Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/fortyforloops.html">
   Intro to For loops
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/nyu-jupyterhub.html">
   NYU JupyterHub Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/ultimate-guide-ttest-python.html">
   Ultimate t-test guide
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../LICENSE.html">
   License
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/08/02-estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://psychua-46-fall.rcnyu.org//hub/user-redirect/git-pull?repo=https://github.com/executablebooks/jupyter-book&urlpath=tree/jupyter-book/chapters/08/02-estimation.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#concrete-population-parameters">
   Concrete population parameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract-population-parameters">
   Abstract population parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#complications-with-inference">
     Complications with inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiments-and-population-parameters">
   Experiments and Population parameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interim-summary">
   Interim summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-the-population-mean">
   Estimating the population mean
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-the-population-standard-deviation">
   Estimating the population standard deviation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-a-confidence-interval">
   Estimating a confidence interval
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-slight-mistake-in-the-formula">
     A slight mistake in the formula
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#videos">
   Videos
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction-to-probability">
     Introduction to Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chebychev-s-theorem">
     Chebychev’s Theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#z-scores">
     Z-scores
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distribution-i">
     Normal Distribution I
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-distribution-ii">
     Normal Distribution II
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="estimating-population-parameters">
<h1>Estimating population parameters<a class="headerlink" href="#estimating-population-parameters" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This chapter is adapted from Danielle Navarro’s excellent <a class="reference external" href="https://learningstatisticswithr.com">Learning Statistics with R</a> book and Matt Crump’s <a class="reference external" href="https://crumplab.github.io/statistics/probability-sampling-and-estimation.html">Answering Questions with Data</a>.  The main text of Matt’s version has mainly be left intact with a few modifications, also the code adapted to use python and jupyter.</p>
</div>
<p>Let’s pause for a moment to get our bearings. We’re about to go into the topic of <strong>estimation</strong>. What is that, and why should you care? First, population parameters are things about a distribution. For example, distributions have means. The mean is a parameter of the distribution. The standard deviation of a distribution is a parameter. Anything that can describe a distribution is a potential parameter.</p>
<p>OK fine, who cares? This I think, is a really good question. There are some good concrete reasons to care. And there are some great abstract reasons to care. Unfortunately, most of the time in research, it’s the abstract reasons that matter most, and these can be the most difficult to get your head around.</p>
<div class="section" id="concrete-population-parameters">
<h2>Concrete population parameters<a class="headerlink" href="#concrete-population-parameters" title="Permalink to this headline">¶</a></h2>
<p>First some concrete reasons. There are real populations out there, and sometimes you want to know the parameters of them. For example, if you are a shoe company, you would want to know about the population parameters of feet size. As a first pass, you would want to know the mean and standard deviation of the population. If your company knew this, and other companies did not, your company would do better (assuming all shoes are made equal). Why would your company do better, and how could it use the parameters? Here’s one good reason. As a shoe company you want to meet demand with the right amount of supply. If you make too many big or small shoes, and there aren’t enough people to buy them, then you’re making extra shoes that don’t sell. If you don’t make enough of the most popular sizes, you’ll be leaving money on the table. Right? Yes. So, what would be an optimal thing to do? Perhaps, you would make different amounts of shoes in each size, corresponding to how the demand for each shoe size. You would know something about the demand by figuring out the frequency of each size in the population. You would need to know the population parameters to do this.</p>
<p>Fortunately, it’s pretty easy to get the population parameters without measuring the entire population. Who has time to measure every-bodies feet? Nobody, that’s who. Instead, you would just need to randomly pick a bunch of people, measure their feet, and then measure the parameters of the sample. If you take a big enough sample, we have learned that the sample mean gives a very good estimate of the population mean. We will learn shortly that a version of the standard deviation of the sample also gives a good estimate of the standard deviation of the population. Perhaps shoe-sizes have a slightly different shape than a normal distribution. Here too, if you collect a big enough sample, the shape of the distribution of the sample will be a good estimate of the shape of the populations. All of these are good reasons to care about estimating population parameters. But, do you run a shoe company? Probably not.</p>
</div>
<div class="section" id="abstract-population-parameters">
<h2>Abstract population parameters<a class="headerlink" href="#abstract-population-parameters" title="Permalink to this headline">¶</a></h2>
<p>Even when we think we are talking about something concrete in Psychology, it often gets abstract right away. Instead of measuring the population of feet-sizes, how about the population of human happiness. We all think we know what happiness is, everyone has more or less of it, there are a bunch of people, so there must be a population of happiness right? Perhaps, but it’s not very concrete. The first problem is figuring out how to measure happiness. Let’s use a questionnaire. Consider these questions:</p>
<blockquote>
<div><p>How happy are you right now on a scale from 1 to 7?
How happy are you in general on a scale from 1 to 7?
How happy are you in the mornings on a scale from 1 to 7?
How happy are you in the afternoons on a scale from 1 to 7?</p>
</div></blockquote>
<ol class="simple">
<li><p>= very unhappy</p></li>
<li><p>= unhappy</p></li>
<li><p>= sort of unhappy</p></li>
<li><p>= in the middle</p></li>
<li><p>= sort of happy</p></li>
<li><p>= happy</p></li>
<li><p>= very happy</p></li>
</ol>
<p>Forget about asking these questions to everybody in the world. Let’s just ask them to lots of people (our sample). What do you think would happen? Well, obviously people would give all sorts of answers right. We could tally up the answers and plot them in a histogram. This would show us a distribution of happiness scores from our sample. “Great, fantastic!”, you say. Yes, fine and dandy.</p>
<p>So, on the one hand we could say lots of things about the people in our sample. We could say exactly who says they are happy and who says they aren’t, after all they just told us!</p>
<p>But, what can we say about the larger population? Can we use the parameters of our sample (e.g., mean, standard deviation, shape etc.) to estimate something about a larger population. Can we infer how happy everybody else is, just from our sample? HOLD THE PHONE.</p>
<div class="section" id="complications-with-inference">
<h3>Complications with inference<a class="headerlink" href="#complications-with-inference" title="Permalink to this headline">¶</a></h3>
<p>Before listing a bunch of complications, let me tell you what I think we can do with our sample. Provided it is big enough, our sample parameters will be a pretty good estimate of what another sample would look like. Because of the following discussion, this is often all we can say. But, that’s OK, as you see throughout this book, we can work with that!</p>
<p><strong>Problem 1: Multiple populations</strong>: If you looked at a large sample of questionnaire data you will find evidence of multiple distributions inside your sample. People answer questions differently. Some people are very cautious and not very extreme. Their answers will tend to be distributed about the middle of the scale, mostly 3s, 4s, and 5s. Some people are very bi-modal, they are very happy and very unhappy, depending on time of day. These people’s answers will be mostly 1s and 2s, and 6s and 7s, and those numbers look like they come from a completely different distribution. Some people are entirely happy or entirely unhappy. Again, these two “populations” of people’s numbers look like two different distributions, one with mostly 6s and 7s, and one with mostly 1s and 2s. Other people will be more random, and their scores will look like a uniform distribution. So, is there a single population with parameters that we can estimate from our sample? Probably not. Could be a mixture of lots of populations with different distributions.</p>
<p><strong>Problem 2: What do these questions measure?</strong>: If the whole point of doing the questionnaire is to estimate the population’s happiness, we really need wonder if the sample measurements actually tell us anything about happiness in the first place. Some questions: Are people accurate in saying how happy they are? Does the measure of happiness depend on the scale, for example, would the results be different if we used 0-100, or -100 to +100, or no numbers? Does the measure of happiness depend on the wording in the question? Does a measure like this one tell us everything we want to know about happiness (probably not), what is it missing (who knows? probably lots). In short, nobody knows if these kinds of questions measure what we want them to measure. We just hope that they do. Instead, we have a very good idea of the kinds of things that they actually measure. It’s really quite obvious, and staring you in the face. Questionnaire measurements measure how people answer questionnaires. In other words, how people behave and answer questions when they are given a questionnaire. This might also measure something about happiness, when the question has to do about happiness. But, it turns out people are remarkably consistent in how they answer questions, even when the questions are total nonsense, or have no questions at all (just numbers to choose!) &#64;maul_rethinking_2017.</p>
<p>The take home complications here are that we can collect samples, but in Psychology, we often don’t have a good idea of the populations that might be linked to these samples. There might be lots of populations, or the populations could be different depending on who you ask. Finally, the “population” might not be the one you want it to be.</p>
</div>
</div>
<div class="section" id="experiments-and-population-parameters">
<h2>Experiments and Population parameters<a class="headerlink" href="#experiments-and-population-parameters" title="Permalink to this headline">¶</a></h2>
<p>OK, so we don’t own a shoe company, and we can’t really identify the population of interest in Psychology, can’t we just skip this section on estimation? After all, the “population” is just too weird and abstract and useless and contentious. HOLD THE PHONE AGAIN!</p>
<p>It turns out we can apply the things we have been learning to solve lots of important problems in research. These allow us to answer questions with the data that we collect. Parameter estimation is one of these tools. We just need to be a little bit more creative, and a little bit more abstract to use the tools.</p>
<p>Here is what we know already. The numbers that we measure come from somewhere, we have called this place “distributions”. Distributions control how the numbers arrive. Some numbers happen more than others depending on the distribution. We assume, even if we don’t know what the distribution is, or what it means, that the numbers came from one. Second, when get some numbers, we call it a sample. This entire chapter so far has taught you one thing. When your sample is big, it resembles the distribution it came from. And, when your sample is big, it will resemble very closely what another big sample of the same thing will look like. We can use this knowledge!</p>
<p>Very often as Psychologists what we want to know is what causes what. We want to know if X causes something to change in Y. Does eating chocolate make you happier? Does studying improve your grades? There a bazillions of these kinds of questions. And, we want answers to them.</p>
<p>I’ve been trying to be mostly concrete so far in this textbook, that’s why we talk about silly things like chocolate and happiness, at least they are concrete. Let’s give a go at being abstract. We can do it.</p>
<p>So, we want to know if X causes Y to change. What is X? What is Y? X is something you change, something you manipulate, the independent variable. Y is something you measure. So, we will be taking samples from Y. “Oh I get it, we’ll take samples from Y, then we can use the sample parameters to estimate the population parameters of Y!” NO, not really, but yes sort of. We will take sample from Y, that is something we absolutely do. In fact, that is really all we ever do, which is why talking about the population of Y is kind of meaningless. We’re more interested in our samples of Y, and how they behave.</p>
<p>So, what would happen if we removed X from the universe altogether, and then took a big sample of Y. We’ll pretend Y measures something in a Psychology experiment. So, we know right away that Y is variable. When we take a big sample, it will have a distribution (because Y is variable). So, we can do things like measure the mean of Y, and measure the standard deviation of Y, and anything else we want to know about Y. Fine. What would happen if we replicated this measurement. That is, we just take another random sample of Y, just as big as the first. What should happen is that our first sample should look a lot like our second example. After all, we didn’t do anything to Y, we just took two big samples twice. Both of our samples will be a little bit different (due to sampling error), but they’ll be mostly the same. The bigger our samples, the more they will look the same, especially when we don’t do anything to cause them to be different. In other words, we can use the parameters of one sample to estimate the parameters of a second sample, because they will tend to be the same, especially when they are large.</p>
<p>We are now ready for step two. You want to know if X changes Y. What do you do? You make X go up and take a big sample of Y then look at it. You make X go down, then take a second big sample of Y and look at it. Next, you compare the two samples of Y. If X does nothing then what should you find? We already discussed that in the previous paragraph. If X does nothing, then both of your big samples of Y should be pretty similar. However, if X does something to Y, then one of your big samples of Y will be different from the other. You will have changed something about Y. Maybe X makes the mean of Y change. Or maybe X makes the variation in Y change. Or, maybe X makes the whole shape of the distribution change. If we find any big changes that can’t be explained by sampling error, then we can conclude that something about X caused a change in Y! We could use this approach to learn about what causes what!</p>
<p>The very important idea is still about estimation, just not population parameter estimation exactly. We know that when we take samples they naturally vary. So, when we estimate a parameter of a sample, like the mean, we know we are off by some amount. When we find that two samples are different, we need to find out if the size of the difference is consistent with what sampling error can produce, or if the difference is bigger than that. If the difference is bigger, then we can be confident that sampling error didn’t produce the difference. So, we can confidently infer that something else (like an X) did cause the difference. This bit of abstract thinking is what most of the rest of the textbook is about. Determining whether there is a difference caused by your manipulation. There’s more to the story, there always is. We can get more specific than just, is there a difference, but for introductory purposes, we will focus on the finding of differences as a foundational concept.</p>
</div>
<div class="section" id="interim-summary">
<h2>Interim summary<a class="headerlink" href="#interim-summary" title="Permalink to this headline">¶</a></h2>
<p>We’ve talked about estimation without doing any estimation, so in the next section we will do some estimating of the mean and of the standard deviation. Formally, we talk about this as using a sample to estimate a parameter of the population. Feel free to think of the “population” in different ways. It could be concrete population, like the distribution of feet-sizes. Or, it could be something more abstract, like the parameter estimate of what samples usually look like when they come from a distribution.</p>
</div>
<div class="section" id="estimating-the-population-mean">
<h2>Estimating the population mean<a class="headerlink" href="#estimating-the-population-mean" title="Permalink to this headline">¶</a></h2>
<p>Suppose we go to Brooklyn and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be <span class="math notranslate nohighlight">\(\bar{X}=98.5\)</span>. So what is the true mean IQ for the entire population of Brooklyn? Obviously, we don’t know the answer to that question. It could be <span class="math notranslate nohighlight">\(97.2\)</span>, but if could also be <span class="math notranslate nohighlight">\(103.5\)</span>. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless if forced to give a “best guess” I’d have to say <span class="math notranslate nohighlight">\(98.5\)</span>. That’s the essence of statistical estimation: giving a best guess. We’re using the sample mean as the best guess of the population mean.</p>
<p>In this example, estimating the unknown population parameter is straightforward. I calculate the sample mean, and I use that as my <strong>estimate of the population mean</strong>. It’s pretty simple, and in the next section we’ll explain the statistical justification for this intuitive answer. However, for the moment let’s make sure you recognize that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often use different notation to refer to them. For instance, if true population mean is denoted <span class="math notranslate nohighlight">\(\mu\)</span>, then we would use <span class="math notranslate nohighlight">\(\hat\mu\)</span> to refer to our estimate of the population mean. In contrast, the sample mean is denoted <span class="math notranslate nohighlight">\(\bar{X}\)</span> or sometimes <span class="math notranslate nohighlight">\(m\)</span>. However, in simple random samples, the estimate of the population mean is identical to the sample mean: if I observe a sample mean of <span class="math notranslate nohighlight">\(\bar{X} = 98.5\)</span>, then my estimate of the population mean is also <span class="math notranslate nohighlight">\(\hat\mu = 98.5\)</span>. To help keep the notation clear, here’s a handy table:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>What is it?</p></th>
<th class="head"><p>Do we know what it is?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\bar{X}\)</span></p></td>
<td><p>Sample mean</p></td>
<td><p>Yes, calculated from the raw data</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p>True population mean</p></td>
<td><p>Almost never known for sure</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\hat{\mu}\)</span></p></td>
<td><p>Estimate of the population mean</p></td>
<td><p>Yes, identical to the sample mean</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="estimating-the-population-standard-deviation">
<h2>Estimating the population standard deviation<a class="headerlink" href="#estimating-the-population-standard-deviation" title="Permalink to this headline">¶</a></h2>
<p>So far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean, our estimate of the population parameter (i.e. <span class="math notranslate nohighlight">\(\hat\mu\)</span>) turned out to identical to the corresponding sample statistic (i.e. <span class="math notranslate nohighlight">\(\bar{X}\)</span>). However, that’s not always true. To see this, let’s have a think about how to construct an <strong>estimate of the population standard deviation</strong>, which we’ll denote <span class="math notranslate nohighlight">\(\hat\sigma\)</span>. What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.</p>
<p>Here’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the <strong>cromulence</strong> of my shoes. It turns out that my shoes have a cromulence of 20. So here’s my sample:</p>
<p><code class="docutils literal notranslate"><span class="pre">20</span></code></p>
<p>This is a perfectly legitimate sample, even if it does have a sample size of <span class="math notranslate nohighlight">\(N=1\)</span>. It has a sample mean of 20, and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the <strong>sample</strong> this seems quite right: the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of <span class="math notranslate nohighlight">\(s = 0\)</span> is the right answer here. But as an estimate of the <strong>population</strong> standard deviation, it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data: the only reason that we don’t see any variability in the <strong>sample</strong> is that the sample is too small to display any variation! So, if you have a sample size of <span class="math notranslate nohighlight">\(N=1\)</span>, it <strong>feels</strong> like the right answer is just to say “no idea at all”.</p>
<p>Notice that you <strong>don’t</strong> have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean, it doesn’t feel completely insane to guess that the population mean is 20. Sure, you probably wouldn’t feel very confident in that guess, because you have only the one observation to work with, but it’s still the best guess you can make.</p>
<p>Let’s extend this example a little. Suppose I now make a second observation. My data set now has <span class="math notranslate nohighlight">\(N=2\)</span> observations of the cromulence of shoes, and the complete sample now looks like this:</p>
<p><code class="docutils literal notranslate"><span class="pre">20,</span> <span class="pre">22</span></code></p>
<p>This time around, our sample is <strong>just</strong> large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is <span class="math notranslate nohighlight">\(\bar{X}=21\)</span>, and the sample standard deviation is <span class="math notranslate nohighlight">\(s=1\)</span>. What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean: if forced to guess, we’d probably guess that the population mean cromulence is 21. What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations, we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is <strong>wrong</strong>: after all, with only two observations we expect it to be wrong to some degree. The worry is that the error is <strong>systematic</strong>.</p>
<p>If the error is systematic, that means it is <strong>biased</strong>. For example, imagine if the sample mean was always smaller than the population mean. If this was true (it’s not), then we couldn’t use the sample mean as an estimator. It would be biased, we’d be using the wrong number.</p>
<p>It turns out the sample standard deviation is a <strong>biased estimator</strong> of the population standard deviation. We can sort of anticipate this by what we’ve been discussing. When the sample size is 1, the standard deviation is 0, which is obviously to small. When the sample size is 2, the standard deviation becomes a number bigger than 0, but because we only have two sample, we suspect it might still be too small. Turns out this intuition is correct.</p>
<p>It would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is use R to simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is 100 and the standard deviation is 15. I can use the <strong>rnorm()</strong> function to generate the the results of an experiment in which I measure <span class="math notranslate nohighlight">\(N=2\)</span> IQ scores, and calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the <strong>sampling distribution of the standard deviation</strong>. I’ve plotted this distribution in Figure &#64;ref(fig:sampdistsd).</p>
<img src="../../images/navarro_img/estimation/sampdistsd.pdf" alt="normal" width="550"/>
<div class="caption" markdown='1'>  
<p>The sampling distribution of the sample standard deviation for a two IQ scores experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram, the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation.”, echo=FALSE,dev=’png’,eval=T}</p>
</div><p>Even though the true population standard deviation is 15, the average of the <strong>sample</strong> standard deviations is only 8.5. Notice that this is a very different from when we were plotting sampling distributions of the sample mean, those were always centered around the mean of the population.</p>
<p>Now let’s extend the simulation. Instead of restricting ourselves to the situation where we have a sample size of <span class="math notranslate nohighlight">\(N=2\)</span>, let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the following results.</p>
<p>Figure &#64;ref(fig:estimatorbiasA) shows the sample mean as a function of sample size. Notice it’s a flat line. The sample mean doesn’t underestimate or overestimate the population mean. It is an unbiased estimate!</p>
<img src="../../images/navarro_img/estimation/biasMean-eps-converted-to.pdf" alt="normal" width="550"/>
<div class="caption" markdown='1'>  
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>An illustration of the fact that the sample mean is an unbiased estimator of the population mean.
</pre></div>
</div>
</div><p>Figure &#64;ref(fig:estimatorbiasB) shows the sample standard deviation as a function of sample size. Notice it is not a flat line. The sample standard deviation systematically underestimates the population standard deviation!</p>
<img src="../../images/navarro_img/estimation/biasSD-eps-converted-to.pdf" alt="normal" width="550"/>
<div class="caption" markdown='1'>  
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>An illustration of the fact that the the sample standard deviation is a biased estimator of the population standard deviation
</pre></div>
</div>
</div><p>In other words, if we want to make a “best guess” (<span class="math notranslate nohighlight">\(\hat\sigma\)</span>, our estimate of the population standard deviation) about the value of the population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, we should make sure our guess is a little bit larger than the sample standard deviation <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>The fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation, let’s look at the variance. If you recall from the second chapter, the sample variance is defined to be the average of the squared deviations from the sample mean. That is: $<span class="math notranslate nohighlight">\(s^2 = \frac{1}{N} \sum_{i=1}^N (X_i - \bar{X})^2\)</span><span class="math notranslate nohighlight">\( The sample variance \)</span>s^2<span class="math notranslate nohighlight">\( is a biased estimator of the population variance \)</span>\sigma^2<span class="math notranslate nohighlight">\(. But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \)</span>N-1<span class="math notranslate nohighlight">\( rather than by \)</span>N<span class="math notranslate nohighlight">\(. If we do that, we obtain the following formula: \)</span><span class="math notranslate nohighlight">\(\hat\sigma^2 = \frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2\)</span><span class="math notranslate nohighlight">\( This is an unbiased estimator of the population variance \)</span>\sigma$.</p>
<p>A similar story applies for the standard deviation. If we divide by <span class="math notranslate nohighlight">\(N-1\)</span> rather than <span class="math notranslate nohighlight">\(N\)</span>, our estimate of the population standard deviation becomes: $<span class="math notranslate nohighlight">\(\hat\sigma = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2}\)</span>$.</p>
<p>It is worth pointing out that software programs make assumptions <strong>for you</strong>, about which variance and standard deviation <strong>you</strong> are computing. Some programs automatically divide by <span class="math notranslate nohighlight">\(N-1\)</span>, some do not. You need to check to figure out what they are doing. Don’t let the software tell you what to do. Software is for you telling it what to do.m</p>
<p>One final point: in practice, a lot of people tend to refer to <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> (i.e., the formula where we divide by <span class="math notranslate nohighlight">\(N-1\)</span>) as the <strong>sample</strong> standard deviation. Technically, this is incorrect: the <strong>sample</strong> standard deviation should be equal to <span class="math notranslate nohighlight">\(s\)</span> (i.e., the formula where we divide by <span class="math notranslate nohighlight">\(N\)</span>). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application, what we actually care about is the estimate of the population parameter, and so people always report <span class="math notranslate nohighlight">\(\hat\sigma\)</span> rather than <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>Note, whether you should divide by N or N-1 also depends on your philosophy about what you are doing. For example, if you don’t think that what you are doing is estimating a population parameter, then why would you divide by N-1? Also, when N is large, it doesn’t matter too much. The difference between a big N, and a big N-1, is just -1.</p>
<p>This is the right number to report, of course, it’s that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two <strong>concepts</strong> separate: it’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(\hat\sigma\)</span> are the same thing, you start doing exactly that.</p>
<p>To finish this section off, here’s another couple of tables to help keep things clear:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>What is it?</p></th>
<th class="head"><p>Do we know what it is?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(s^2\)</span></p></td>
<td><p>Sample variance</p></td>
<td><p>Yes, calculated from the raw data</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma^2\)</span></p></td>
<td><p>Population variance</p></td>
<td><p>Almost never known for sure</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span></p></td>
<td><p>Estimate of the population variance</p></td>
<td><p>Yes, but not the same as the sample variance</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="estimating-a-confidence-interval">
<h2>Estimating a confidence interval<a class="headerlink" href="#estimating-a-confidence-interval" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>Statistics means never having to say you’re certain – Unknown origin</p>
</div></blockquote>
<p>Up to this point in this chapter, we’ve outlined the basics of sampling theory which statisticians rely on to make guesses about population parameters on the basis of a sample of data. As this discussion illustrates, one of the reasons we need all this sampling theory is that every data set leaves us with some of uncertainty, so our estimates are never going to be perfectly accurate. The thing that has been missing from this discussion is an attempt to <strong>quantify</strong> the amount of uncertainty in our estimate. It’s not enough to be able guess that the mean IQ of undergraduate psychology students is 115 (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a 95% chance that the true mean lies between 109 and 121. The name for this is a <strong>confidence interval</strong> for the mean.</p>
<p>Armed with an understanding of sampling distributions, constructing a confidence interval for the mean is actually pretty easy. Here’s how it works. Suppose the true population mean is <span class="math notranslate nohighlight">\(\mu\)</span> and the standard deviation is <span class="math notranslate nohighlight">\(\sigma\)</span>. I’ve just finished running my study that has <span class="math notranslate nohighlight">\(N\)</span> participants, and the mean IQ among those participants is <span class="math notranslate nohighlight">\(\bar{X}\)</span>. We know from our discussion of the central limit theorem that the sampling distribution of the mean is approximately normal. We also know from our discussion of the normal distribution that there is a 95% chance that a normally-distributed quantity will fall within two standard deviations of the true mean. To be more precise, we can use the <strong>qnorm()</strong> function to compute the 2.5th and 97.5th percentiles of the normal distribution</p>
<blockquote>
<div><p>qnorm( p = c(.025, .975) ) [1] -1.959964 1.959964</p>
</div></blockquote>
<p>Okay, so I lied earlier on. The more correct answer is that a 95% chance that a normally-distributed quantity will fall within 1.96 standard deviations of the true mean.</p>
<p>Next, recall that the standard deviation of the sampling distribution is referred to as the standard error, and the standard error of the mean is written as SEM. When we put all these pieces together, we learn that there is a 95% probability that the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> that we have actually observed lies within 1.96 standard errors of the population mean. Oof, that is a lot of mathy talk there. We’ll clear it up, don’t worry.</p>
<p>Mathematically, we write this as: <span class="math notranslate nohighlight">\(\mu - \left( 1.96 \times \mbox{SEM} \right) \ \leq \  \bar{X}\  \leq \  \mu + \left( 1.96 \times \mbox{SEM} \right)\)</span> where the SEM is equal to <span class="math notranslate nohighlight">\(\sigma / \sqrt{N}\)</span>, and we can be 95% confident that this is true.</p>
<p>However, that’s not answering the question that we’re actually interested in. The equation above tells us what we should expect about the sample mean, given that we know what the population parameters are. What we <strong>want</strong> is to have this work the other way around: we want to know what we should believe about the population parameters, given that we have observed a particular sample. However, it’s not too difficult to do this. Using a little high school algebra, a sneaky way to rewrite our equation is like this: <span class="math notranslate nohighlight">\(\bar{X} -  \left( 1.96 \times \mbox{SEM} \right) \ \leq \ \mu  \ \leq  \ \bar{X} +  \left( 1.96 \times \mbox{SEM}\right)\)</span> What this is telling is is that the range of values has a 95% probability of containing the population mean <span class="math notranslate nohighlight">\(\mu\)</span>. We refer to this range as a <strong>95% confidence interval</strong>, denoted <span class="math notranslate nohighlight">\(\mbox{CI}_{95}\)</span>. In short, as long as <span class="math notranslate nohighlight">\(N\)</span> is sufficiently large – large enough for us to believe that the sampling distribution of the mean is normal – then we can write this as our formula for the 95% confidence interval: <span class="math notranslate nohighlight">\(\mbox{CI}_{95} = \bar{X} \pm \left( 1.96 \times \frac{\sigma}{\sqrt{N}} \right)\)</span> Of course, there’s nothing special about the number 1.96: it just happens to be the multiplier you need to use if you want a 95% confidence interval. If I’d wanted a 70% confidence interval, I could have used the <strong>qnorm()</strong> function to calculate the 15th and 85th quantiles:</p>
<blockquote>
<div><p>qnorm( p = c(.15, .85) ) [1] -1.036433 1.036433</p>
</div></blockquote>
<p>and so the formula for <span class="math notranslate nohighlight">\(\mbox{CI}_{70}\)</span> would be the same as the formula for <span class="math notranslate nohighlight">\(\mbox{CI}_{95}\)</span> except that we’d use 1.04 as our magic number rather than 1.96.</p>
<div class="section" id="a-slight-mistake-in-the-formula">
<h3>A slight mistake in the formula<a class="headerlink" href="#a-slight-mistake-in-the-formula" title="Permalink to this headline">¶</a></h3>
<p>As usual, I lied. The formula that I’ve given above for the 95% confidence interval is approximately correct, but I glossed over an important detail in the discussion. Notice my formula requires you to use the standard error of the mean, SEM, which in turn requires you to use the true population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>Yet, before we stressed the fact that we don’t actually <strong>know</strong> the true population parameters. Because we don’t know the true value of <span class="math notranslate nohighlight">\(\sigma\)</span>, we have to use an estimate of the population standard deviation <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> instead. This is pretty straightforward to do, but this has the consequence that we need to use the quantiles of the <span class="math notranslate nohighlight">\(t\)</span>-distribution rather than the normal distribution to calculate our magic number; and the answer depends on the sample size. Plus, we haven’t really talked about the <span class="math notranslate nohighlight">\(t\)</span> distribution yet.</p>
<p>When we use the <span class="math notranslate nohighlight">\(t\)</span> distribution instead of the normal distribution, we get bigger numbers, indicating that we have more uncertainty. And why do we have that extra uncertainty? Well, because our estimate of the population standard deviation <span class="math notranslate nohighlight">\(\hat\sigma\)</span> might be wrong! If it’s wrong, it implies that we’re a bit less sure about what our sampling distribution of the mean actually looks like… and this uncertainty ends up getting reflected in a wider confidence interval.</p>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this chapter and the two before we’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:</p>
<ul class="simple">
<li><p>Basic ideas about samples, sampling and populations</p></li>
<li><p>Statistical theory of sampling: the law of large numbers, sampling distributions and the central limit theorem.</p></li>
<li><p>Estimating means and standard deviations</p></li>
<li><p>Confidence intervals</p></li>
</ul>
<p>As always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.</p>
</div>
<div class="section" id="videos">
<h2>Videos<a class="headerlink" href="#videos" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction-to-probability">
<h3>Introduction to Probability<a class="headerlink" href="#introduction-to-probability" title="Permalink to this headline">¶</a></h3>
<p>Jeff has several more videos on probability that you can view on his <a class="reference external" href="https://www.youtube.com/playlist?list=PLKXdxQAT3tCvuex_E1ZnQYaw897ELUSaI">statistics playlist</a>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/RoalMn9VHZg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="section" id="chebychev-s-theorem">
<h3>Chebychev’s Theorem<a class="headerlink" href="#chebychev-s-theorem" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/4RtwqCFt1IU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="section" id="z-scores">
<h3>Z-scores<a class="headerlink" href="#z-scores" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/dQVO0KAxFaU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="section" id="normal-distribution-i">
<h3>Normal Distribution I<a class="headerlink" href="#normal-distribution-i" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/FPJDF9fGwwE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<div class="section" id="normal-distribution-ii">
<h3>Normal Distribution II<a class="headerlink" href="#normal-distribution-ii" title="Permalink to this headline">¶</a></h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/mPdnF-GVuCo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/08"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Todd Gureckis<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>