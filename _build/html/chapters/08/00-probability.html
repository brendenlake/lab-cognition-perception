
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Probability &#8212; Lab in C&amp;P (Fall 2021)</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/nyustyle.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/artificialintelligence.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Lab in C&P (Fall 2021)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Overview
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../course-content/syllabus.html">
   Syllabus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../course-content/schedule.html">
   Schedule
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://edstem.org/us/courses/8295/discussion/">
   EdStem
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Textbook
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00/00-cogsci.html">
   1. What is Cognitive Science and how do we study it?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../01/00-whystats.html">
   2. Why do we have to learn statistics?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/00-jupyter.html">
   3. Introduction to Jupyter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03/00-python.html">
   4. Intro to Python for Psychology Undergrads
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/00-researchdesign.html">
   5. A brief introduction to research design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/00-data.html">
   6. The Format and Structure of Digital Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06/00-plots.html">
   7. Visualizing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07/00-describingdata.html">
   8. Describing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="01-sampling.html">
   9. Samples, populations and sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/00-hypothesistesting.html">
   10. Hypothesis testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../10/00-ttest.html">
   11. Comparing one or two means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../11/00-inferences-from-behavior.html">
   12. Measuring Behavior
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../21/00-ethics-irb.html">
   13. Research Ethics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../13/00-linearregression.html">
   14. Linear regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../14/00-logisticregression.html">
   15. Logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../15/00-mixed-effect.html">
   16. Linear Mixed Effect Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../16/00-mentalsimulation.html">
   17. Mental Imagery, Mental Simulation, and Mental Rotation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../17/00-mri.html">
   18. Magnetic Resonance Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../24/00-what-next.html">
   19. What Next?
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Labs &amp; Homeworks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00/cogsci-ica.html">
   Intro to CogSci ICA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../homeworks/Homework1.html">
   Intro to Jupyter (HW1)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tips &amp; Tricks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/pythonresources.html">
   Python Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/plottingresources.html">
   Plotting in Python Resources
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/fortyforloops.html">
   Intro to For loops
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/nyu-jupyterhub.html">
   NYU JupyterHub Setup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../tips/ultimate-guide-ttest-python.html">
   Ultimate t-test guide
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../LICENSE.html">
   License
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/08/00-probability.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        
        <a class="jupyterhub-button" href="https://psychua-46-fall.rcnyu.org//hub/user-redirect/git-pull?repo=https://github.com/executablebooks/jupyter-book&urlpath=tree/jupyter-book/chapters/08/00-probability.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="../../_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-are-probability-and-statistics-different">
   How are probability and statistics different?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-does-probability-mean">
   What does probability mean?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-frequentist-view">
     The frequentist view
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-bayesian-view">
     The Bayesian view
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-s-the-difference-and-who-is-right">
     What’s the difference? And who is right?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-probability-theory">
   Basic probability theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introducing-probability-distributions">
     Introducing probability distributions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-binomial-distribution">
   The binomial distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introducing-the-binomial">
     Introducing the binomial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-with-the-binomial-distribution-in-python">
     Working with the binomial distribution in Python
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-normal-distribution">
   The normal distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-density">
     Probability density
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-useful-distributions">
   Other useful distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-of-probability">
   Summary of Probability
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <p>This chapter is adapted from Danielle Navarro’s excellent <a class="reference external" href="https://learningstatisticswithr.com">Learning Statistics with R</a> book and Matt Crump’s <a class="reference external" href="https://crumplab.github.io/statistics/probability-sampling-and-estimation.html">Answering Questions with Data</a>.  The main text of Matt’s version has mainly be left intact with a few modifications, also the code adapted to use python and jupyter.</p>
<div class="section" id="probability">
<h1>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h1>
<p>I have studied many languages-French, Spanish and a little Italian, but no one told me that Statistics was a foreign language. —Charmaine J. Forde</p>
<p>Up to this point in the book, we’ve discussed some of the key ideas in experimental design, and we’ve talked a little about how you can summarize a data set. To a lot of people, this is all there is to statistics: it’s about calculating averages, collecting all the numbers, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting, but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics, and one of the least powerful. The bigger and more useful part of statistics is that it provides tools <strong>that let you make inferences about data</strong>.</p>
<p>Once you start thinking about statistics in these terms – that statistics is there to help us draw inferences from data – you start seeing examples of it everywhere. For instance, here’s a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):</p>
<p>“I have a tough job,” the Premier said in response to a poll which found her government is now the most unpopular Labor administration in polling history, with a primary vote of just 23 per cent.</p>
<p>This kind of remark is entirely unremarkable in the papers or in everyday life, but let’s have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I’m too lazy to track down the original survey, so let’s just imagine that they called 1000 voters at random, and 230 (23%) of those claimed that they intended to vote for the party. For the 2010 Federal election, the Australian Electoral Commission reported 4,610,795 enrolled voters in New South Whales; so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?</p>
<p>The answer to the question is pretty obvious: if I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the <strong>only</strong> 230 people out of the entire voting public who actually intend to do so. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it’s a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.</p>
<p><strong>Inferential statistics</strong> provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, our tools for making statistical inferences are 1) built on top of <strong>probability theory</strong>, and 2) require an understanding of how samples behave when you take them from distributions (defined by probability theory…). So, this chapter has two main parts. A brief introduction to probability theory, and an introduction to sampling from distributions.</p>
<div class="section" id="how-are-probability-and-statistics-different">
<h2>How are probability and statistics different?<a class="headerlink" href="#how-are-probability-and-statistics-different" title="Permalink to this headline">¶</a></h2>
<p>Before we start talking about probability theory, it’s helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they’re not identical. Probability theory is “the doctrine of chances”. It’s a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:</p>
<ul class="simple">
<li><p>What are the chances of a fair coin coming up heads 10 times in a row?</p></li>
<li><p>If I roll two six sided dice, how likely is it that I’ll roll two sixes?</p></li>
<li><p>How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?</p></li>
<li><p>What are the chances that I’ll win the lottery?</p></li>
</ul>
<p>Notice that all of these questions have something in common. In each case the “truth of the world” is known, and my question relates to the “what kind of events” will happen. In the first question I <strong>know</strong> that the coin is fair, so there’s a 50% chance that any individual coin flip will come up heads. In the second question, I <strong>know</strong> that the chance of rolling a 6 on a single die is 1 in 6. In the third question I <strong>know</strong> that the deck is shuffled properly. And in the fourth question, I <strong>know</strong> that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known <em>model</em> of the world, and we use that model to do some calculations.</p>
<p>The underlying model can be quite simple. For instance, in the coin flipping example, we can write down the model like this: <span class="math notranslate nohighlight">\(P(\mbox{heads}) = 0.5\)</span> which you can read as “the probability of heads is 0.5”.</p>
<p>As we’ll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question, I don’t actually know exactly what’s going to happen. Maybe I’ll get 10 heads, like the question says. But maybe I’ll get three heads. That’s the key thing: in probability theory, the <strong>model</strong> is known, but the <strong>data</strong> are not.</p>
<p>So that’s probability. What about statistics? Statistical questions work the other way around. In statistics, we know the truth about the world. All we have is the data, and it is from the data that we want to <strong>learn</strong> the truth about the world. Statistical questions tend to look more like these:</p>
<ul class="simple">
<li><p>If my friend flips a coin 10 times and gets 10 heads, are they playing a trick on me?</p></li>
<li><p>If five cards off the top of the deck are all hearts, how likely is it that the deck was shuffled?</p></li>
<li><p>If the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?</p></li>
</ul>
<p>This time around, the only thing we have are data. What I <strong>know</strong> is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to <em>infer</em> is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>H H H H H H H H H H H
</pre></div>
</div>
<p>and what I’m trying to do is work out which “model of the world” I should put my trust in. If the coin is fair, then the model I should adopt is one that says that the probability of heads is 0.5; that is, <span class="math notranslate nohighlight">\(P(\mbox{heads}) = 0.5\)</span>. If the coin is not fair, then I should conclude that the probability of heads is <strong>not</strong> 0.5, which we would write as <span class="math notranslate nohighlight">\(P(\mbox{heads}) \neq 0.5\)</span>. In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works.</p>
</div>
<div class="section" id="what-does-probability-mean">
<h2>What does probability mean?<a class="headerlink" href="#what-does-probability-mean" title="Permalink to this headline">¶</a></h2>
<p>Let’s start with the first of these questions. What is “probability”? It might seem surprising to you, but while statisticians and mathematicians (mostly) agree on what the <strong>rules</strong> of probability are, there’s much less of a consensus on what the word really <strong>means</strong>. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. If you had to explain “probability” to a five year old, you could do a pretty good job. But if you’ve ever had that experience in real life, you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t <strong>really</strong> know what it’s all about.</p>
<p>So I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, <strong>Arduino Arsenal</strong> and <strong>C Milan</strong>. After thinking about it, I decide that there is an 80% probability that <strong>Arduino Arsenal</strong> winning. What do I mean by that? Here are three possibilities…</p>
<ul class="simple">
<li><p>They’re robot teams, so I can make them play over and over again, and if I did that, <strong>Arduino Arsenal</strong> would win 8 out of every 10 games on average.</p></li>
<li><p>For any given game, I would only agree that betting on this game is only “fair” if a <span class="math notranslate nohighlight">\(\$1\)</span> bet on <strong>C Milan</strong> gives a <span class="math notranslate nohighlight">\(\$5\)</span> payoff (i.e. I get my <span class="math notranslate nohighlight">\(\$1\)</span> back plus a <span class="math notranslate nohighlight">\(\$4\)</span> reward for being correct), as would a <span class="math notranslate nohighlight">\(\$4\)</span> bet on <strong>Arduino Arsenal</strong> (i.e., my <span class="math notranslate nohighlight">\(\$4\)</span> bet plus a <span class="math notranslate nohighlight">\(\$1\)</span> reward).</p></li>
<li><p>My subjective “belief” or “confidence” in an <strong>Arduino Arsenal</strong> victory is four times as strong as my belief in a <strong>C Milan</strong> victory.</p></li>
</ul>
<p>Each of these seems sensible. However they’re not identical, and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section, I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.</p>
<div class="section" id="the-frequentist-view">
<h3>The frequentist view<a class="headerlink" href="#the-frequentist-view" title="Permalink to this headline">¶</a></h3>
<p>The first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the <em>frequentist view</em>, and it defines probability as a <em>long-run frequency</em>. Suppose we were to try flipping a fair coin, over and over again. By definition, this is a coin that has <span class="math notranslate nohighlight">\(P(H) = 0.5\)</span>. What might we observe? One possibility is that the first 20 flips might look like this:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>T,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H
</pre></div>
</div>
<p>In this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call <span class="math notranslate nohighlight">\(N_H\)</span>) that I’ve seen, across the first <span class="math notranslate nohighlight">\(N\)</span> flips, and calculate the proportion of heads <span class="math notranslate nohighlight">\(N_H / N\)</span> every time. Here’s what I’d get (I did literally flip coins to produce this!):</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>number of flips</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="head"><p>6</p></th>
<th class="head"><p>7</p></th>
<th class="head"><p>8</p></th>
<th class="head"><p>9</p></th>
<th class="head"><p>10</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>number of heads</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>4</p></td>
<td><p>4</p></td>
<td><p>4</p></td>
<td><p>5</p></td>
<td><p>6</p></td>
<td><p>7</p></td>
</tr>
<tr class="row-odd"><td><p>proportion</p></td>
<td><p>.00</p></td>
<td><p>.50</p></td>
<td><p>.67</p></td>
<td><p>.75</p></td>
<td><p>.80</p></td>
<td><p>.67</p></td>
<td><p>.57</p></td>
<td><p>.63</p></td>
<td><p>.67</p></td>
<td><p>.70</p></td>
</tr>
</tbody>
</table>
<p>continuing…</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>number of flips</p></th>
<th class="head"><p>11</p></th>
<th class="head"><p>12</p></th>
<th class="head"><p>13</p></th>
<th class="head"><p>14</p></th>
<th class="head"><p>15</p></th>
<th class="head"><p>16</p></th>
<th class="head"><p>17</p></th>
<th class="head"><p>18</p></th>
<th class="head"><p>19</p></th>
<th class="head"><p>20</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>number of heads</p></td>
<td><p>8</p></td>
<td><p>8</p></td>
<td><p>9</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>10</p></td>
<td><p>11</p></td>
</tr>
<tr class="row-odd"><td><p>proportion</p></td>
<td><p>.73</p></td>
<td><p>.67</p></td>
<td><p>.69</p></td>
<td><p>.71</p></td>
<td><p>.67</p></td>
<td><p>.63</p></td>
<td><p>.59</p></td>
<td><p>.56</p></td>
<td><p>.53</p></td>
<td><p>.55</p></td>
</tr>
</tbody>
</table>
<p>Notice that at the start of the sequence, the <strong>proportion</strong> of heads fluctuates wildly, starting at .00 and rising as high as .80. Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of .50. This is the frequentist definition of probability in a nutshell: flip a fair coin over and over again, and as <span class="math notranslate nohighlight">\(N\)</span> grows large (approaches infinity, denoted <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span>), the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking, that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins, or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer, and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times, and then drew a picture of what happens to the proportion <span class="math notranslate nohighlight">\(N_H / N\)</span> as <span class="math notranslate nohighlight">\(N\)</span> increases. Actually, I did it four times, just to make sure it wasn’t a fluke. The results are shown in Figure [fig:frequentistprobability]. As you can see, the <strong>proportion of observed heads</strong> eventually stops fluctuating, and settles down; when it does, the number at which it finally settles is the true probability of heads.</p>
<img src="../../images/navarro_img/probability/frequentistProb-eps-converted-to.png" alt="cogsci logo" width="550"/>
<center>
    <b>Figure 1</b>. An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you've seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have.
<center><p>The frequentist definition of probability has some desirable characteristics. First, it is objective: the probability of an event is <strong>necessarily</strong> grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe. Second, it is unambiguous: any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.</p>
<p>However, it also has undesirable characteristics. Infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands, it impacts on the ground. Each impact wears the coin down a bit; eventually, the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything.</p>
<p>More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says, “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only 2 November 2048. There’s no infinite sequence of events here, just a once-off thing. Frequentist probability genuinely <strong>forbids</strong> us from making probability statements about a single event. From the frequentist perspective, it will either rain tomorrow or it will not; there is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like this: “There is a category of days for which I predict a 60% chance of rain; if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counterintuitive to think of it this way, but you do see frequentists do this sometimes.</p>
</div>
<div class="section" id="the-bayesian-view">
<h3>The Bayesian view<a class="headerlink" href="#the-bayesian-view" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Bayesian view</strong> of probability is often called the subjectivist view, and it is a minority view among statisticians, but one that has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the <strong>degree of belief</strong> that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world, but rather in the thoughts and assumptions of people and other intelligent beings. However, in order for this approach to work, we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet: if it rains tomorrow, then I win <span class="math notranslate nohighlight">\(\$5\)</span>, but if it doesn’t rain then I lose <span class="math notranslate nohighlight">\(\$5\)</span>. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40%, then it’s a bad bet to take. Thus, we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.</p>
<p>What are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective – specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician, but there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable: it seems to make probability arbitrary. While the Bayesian approach does require that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs; I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event: when that happens, then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).</p>
</div>
<div class="section" id="what-s-the-difference-and-who-is-right">
<h3>What’s the difference? And who is right?<a class="headerlink" href="#what-s-the-difference-and-who-is-right" title="Permalink to this headline">¶</a></h3>
<p>Now that you’ve seen each of these two views independently, it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian do? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives, you should have some sense of how to answer those questions.</p>
<p>Okay, assuming you understand the different, you might be wondering which of them is <strong>right</strong>? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details, Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.</p>
<p>For the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods, for reasons I’ll explain towards the end of the book, but I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” &#64;Fisher1922b [p. 311]. Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” &#64;Meehl1967 [p. 114]. The history of statistics, as you might gather, is not devoid of entertainment.</p>
</div>
</div>
<div class="section" id="basic-probability-theory">
<h2>Basic probability theory<a class="headerlink" href="#basic-probability-theory" title="Permalink to this headline">¶</a></h2>
<p>Ideological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so, I’m going to have to talk about my pants.</p>
<div class="section" id="introducing-probability-distributions">
<h3>Introducing probability distributions<a class="headerlink" href="#introducing-probability-distributions" title="Permalink to this headline">¶</a></h3>
<p>One of the disturbing truths about my life is that I only own 5 pairs of pants: three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span>, <span class="math notranslate nohighlight">\(X_3\)</span>, <span class="math notranslate nohighlight">\(X_4\)</span> and <span class="math notranslate nohighlight">\(X_5\)</span>. I really do: that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of pants to wear. Not even I’m so stupid as to try to wear two pairs of pants, and thanks to years of training I never go outside without wearing pants anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of pants (i.e., each <span class="math notranslate nohighlight">\(X\)</span>) as an <em>elementary event</em>. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of pants), then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of pants, so my pants satisfy this constraint. Similarly, the set of all possible events is called a <em>sample space</em>. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my pants in probabilistic terms. Sad.</p>
<p>Okay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (pants), what we want to do is assign a <em>probability</em> of one of these elementary events. For an event <span class="math notranslate nohighlight">\(X\)</span>, the probability of that event <span class="math notranslate nohighlight">\(P(X)\)</span> is a number that lies between 0 and 1. The bigger the value of <span class="math notranslate nohighlight">\(P(X)\)</span>, the more likely the event is to occur. So, for example, if <span class="math notranslate nohighlight">\(P(X) = 0\)</span>, it means the event <span class="math notranslate nohighlight">\(X\)</span> is impossible (i.e., I never wear those pants). On the other hand, if <span class="math notranslate nohighlight">\(P(X) = 1\)</span> it means that event <span class="math notranslate nohighlight">\(X\)</span> is certain to occur (i.e., I always wear those pants). For probability values in the middle, it means that I sometimes wear those pants. For instance, if <span class="math notranslate nohighlight">\(P(X) = 0.5\)</span> it means that I wear those pants half of the time.</p>
<p>At this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on pants, I really do end up wearing pants (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the <em>law of total probability</em>, not that any of us really care. More importantly, if these requirements are satisfied, then what we have is a <em>probability distribution</em>. For example, this is an example of a probability distribution</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Which pants?</p></th>
<th class="head"><p>Label</p></th>
<th class="head"><p>Probability</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Blue jeans</p></td>
<td><p><span class="math notranslate nohighlight">\(X_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X_1) = .5\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Grey jeans</p></td>
<td><p><span class="math notranslate nohighlight">\(X_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X_2) = .3\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Black jeans</p></td>
<td><p><span class="math notranslate nohighlight">\(X_3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X_3) = .1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Black suit</p></td>
<td><p><span class="math notranslate nohighlight">\(X_4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X_4) = 0\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Blue tracksuit</p></td>
<td><p><span class="math notranslate nohighlight">\(X_5\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(X_5) = .1\)</span></p></td>
</tr>
</tbody>
</table>
<p>Each of the events has a probability that lies between 0 and 1, and if we add up the probability of all events, they sum to 1. Awesome. We can even draw a nice bar graph to visualise this distribution, as shown in Figure &#64;ref(fig:4pantsprob). And at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my pants. Everyone wins!</p>
<img src="../../images/navarro_img/probability/pantsDistribution-eps-converted-to.png" alt="pants" width="550"/>
<p>The only other thing that I need to point out is that probability theory allows you to talk about <em>non elementary events</em> as well as elementary ones. The easiest way to illustrate the concept is with an example. In the pants example, it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dan wears jeans” event said to have happened as long as the elementary event that actually did occur is one of the appropriate ones; in this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms, we defined the “jeans” event <span class="math notranslate nohighlight">\(E\)</span> to correspond to the set of elementary events <span class="math notranslate nohighlight">\((X_1, X_2, X_3)\)</span>. If any of these elementary events occurs, then <span class="math notranslate nohighlight">\(E\)</span> is also said to have occurred. Having decided to write down the definition of the <span class="math notranslate nohighlight">\(E\)</span> this way, it’s pretty straightforward to state what the probability <span class="math notranslate nohighlight">\(P(E)\)</span> is: we just add everything up. In this particular case $<span class="math notranslate nohighlight">\(P(E) = P(X_1) + P(X_2) + P(X_3)\)</span>$ and, since the probabilities of blue, grey and black jeans respectively are .5, .3 and .1, the probability that I wear jeans is equal to .9.</p>
<p>At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book, I won’t do so here.</p>
<p>Some basic rules that probabilities must satisfy. You don’t really need to know these rules in order to understand the analyses that we’ll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>English</p></th>
<th class="head"><p>Math Notation</p></th>
<th class="head"><p></p></th>
<th class="head"><p>Mathematical Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>not <span class="math notranslate nohighlight">\(A\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(\neg A)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(=\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1-P(A)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(A \cup B)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(=\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(A) + P(B) - P(A \cap B)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(A \cap B)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(=\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(P(A\|B) P(B)\)</span></p></td>
</tr>
</tbody>
</table>
<p>Now that we have the ability to “define” non-elementary events in terms of elementary ones, we can actually use this to construct (or, if you want to be all mathematicallish, “derive”) some of the other rules of probability. These rules are listed above, and while I’m pretty confident that very few of my readers actually care about how these rules are constructed, I’m going to show you anyway: even though it’s boring and you’ll probably never have a lot of use for these derivations, if you read through it once or twice and try to see how it works, you’ll find that probability starts to feel a bit less mysterious, and with any luck a lot less daunting. So here goes. Firstly, in order to construct the rules I’m going to need a sample space <span class="math notranslate nohighlight">\(X\)</span> that consists of a bunch of elementary events <span class="math notranslate nohighlight">\(x\)</span>, and two non-elementary events, which I’ll call <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. Let’s say: $<span class="math notranslate nohighlight">\(\begin{array}{rcl}
X &amp;=&amp; (x_1, x_2, x_3, x_4, x_5) \\
A &amp;=&amp; (x_1, x_2, x_3) \\
B &amp;=&amp; (x_3, x_4)
\end{array}\)</span><span class="math notranslate nohighlight">\( To make this a bit more concrete, let's suppose that we're still talking about the pants distribution. If so, \)</span>A<span class="math notranslate nohighlight">\( corresponds to the event &quot;jeans&quot;, and \)</span>B<span class="math notranslate nohighlight">\( corresponds to the event &quot;black&quot;: \)</span><span class="math notranslate nohighlight">\(\begin{array}{rcl}
\mbox{``jeans''} &amp;=&amp; (\mbox{``blue jeans''}, \mbox{``grey jeans''}, \mbox{``black jeans''}) \\
\mbox{``black''} &amp;=&amp; (\mbox{``black jeans''}, \mbox{``black suit''})
\end{array}\)</span>$ So now let’s start checking the rules that I’ve listed in the table.</p>
<p>In the first line, the table says that $<span class="math notranslate nohighlight">\(P(\neg A) = 1- P(A)\)</span><span class="math notranslate nohighlight">\( and what it **means** is that the probability of &quot;not \)</span>A<span class="math notranslate nohighlight">\(&quot; is equal to 1 minus the probability of \)</span>A<span class="math notranslate nohighlight">\(. A moment's thought (and a tedious example) make it obvious why this must be true. If \)</span>A<span class="math notranslate nohighlight">\( coresponds to the even that I wear jeans (i.e., one of \)</span>x_1<span class="math notranslate nohighlight">\( or \)</span>x_2<span class="math notranslate nohighlight">\( or \)</span>x_3<span class="math notranslate nohighlight">\( happens), then the only meaningful definitionof &quot;not \)</span>A<span class="math notranslate nohighlight">\(&quot; (which is mathematically denoted as \)</span>\neg A<span class="math notranslate nohighlight">\() is to say that \)</span>\neg A<span class="math notranslate nohighlight">\( consists of **all** elementary events that don't belong to \)</span>A<span class="math notranslate nohighlight">\(. In the case of the pants distribution it means that \)</span>\neg A = (x_4, x_5)<span class="math notranslate nohighlight">\(, or, to say it in English: &quot;not jeans&quot; consists of all pairs of pants that aren't jeans (i.e., the black suit and the blue tracksuit). Consequently, every single elementary event belongs to either \)</span>A<span class="math notranslate nohighlight">\( or \)</span>\neg A<span class="math notranslate nohighlight">\(, but not both. Okay, so now let's rearrange our statement above: \)</span><span class="math notranslate nohighlight">\(P(\neg A) + P(A) = 1\)</span><span class="math notranslate nohighlight">\( which is a trite way of saying either I do wear jeans or I don't wear jeans: the probability of &quot;not jeans&quot; plus the probability of &quot;jeans&quot; is 1. Mathematically: \)</span><span class="math notranslate nohighlight">\(\begin{array}{rcl}
P(\neg A) &amp;=&amp;  P(x_4) + P(x_5) \\
P(A) &amp;=&amp; P(x_1) + P(x_2) + P(x_3) 
\end{array}\)</span><span class="math notranslate nohighlight">\( so therefore \)</span><span class="math notranslate nohighlight">\(\begin{array}{rcl} 
P(\neg A) + P(A) &amp;=&amp; P(x_1) + P(x_2) + P(x_3) + P(x_4) + P(x_5) \\
&amp;=&amp; \sum_{x \in X} P(x) \\
&amp;=&amp; 1
\end{array}\)</span>$ Excellent. It all seems to work.</p>
<p>Wow, I can hear you saying. That’s a lot of <span class="math notranslate nohighlight">\(x\)</span>s to tell me the freaking obvious. And you’re right: this <strong>is</strong> freaking obvious. The whole <strong>point</strong> of probability theory to to formalise and mathematise a few very basic common sense intuitions. So let’s carry this line of thought forward a bit further. In the last section I defined an event corresponding to <strong>not</strong> A, which I denoted <span class="math notranslate nohighlight">\(\neg A\)</span>. Let’s now define two new events that correspond to important everyday concepts: <span class="math notranslate nohighlight">\(A\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(A\)</span> <strong>or</strong> <span class="math notranslate nohighlight">\(B\)</span>. To be precise:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>English statement:</p></th>
<th class="head"><p>Mathematical notation:</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>“<span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>” both happen</p></td>
<td><p><span class="math notranslate nohighlight">\(A \cap B\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>at least one of “<span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span>” happens</p></td>
<td><p><span class="math notranslate nohighlight">\(A \cup B\)</span></p></td>
</tr>
</tbody>
</table>
<p>Since <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are both defined in terms of our elementary events (the <span class="math notranslate nohighlight">\(x\)</span>s) we’re going to need to try to describe <span class="math notranslate nohighlight">\(A \cap B\)</span> and <span class="math notranslate nohighlight">\(A \cup B\)</span> in terms of our elementary events too. Can we do this? Yes we can The only way that both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> can occur is if the elementary event that we observe turns out to belong to both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. Thus “<span class="math notranslate nohighlight">\(A \cap B\)</span>” includes only those elementary events that belong to both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>… $<span class="math notranslate nohighlight">\(\begin{array}{rcl}
A &amp;=&amp; (x_1, x_2, x_3) \\
B &amp;=&amp; (x_3, x_4) \\
A \cap B &amp; = &amp; (x_3) 
\end{array}\)</span><span class="math notranslate nohighlight">\( So, um, the only way that I can wear &quot;jeans&quot; \)</span>(x_1, x_2, x_3)<span class="math notranslate nohighlight">\( and &quot;black pants&quot; \)</span>(x_3, x_4)<span class="math notranslate nohighlight">\( is if I wear &quot;black jeans&quot; \)</span>(x_3)$. Another victory for the bloody obvious.</p>
<p>At this point, you’re not going to be at all shocked by the definition of <span class="math notranslate nohighlight">\(A \cup B\)</span>, though you’re probably going to be extremely bored by it. The only way that I can wear “jeans” or “black pants” is if the elementary pants that I actually do wear belongs to <span class="math notranslate nohighlight">\(A\)</span> or to <span class="math notranslate nohighlight">\(B\)</span>, or to both. So… $<span class="math notranslate nohighlight">\(\begin{array}{rcl}
A &amp;=&amp; (x_1, x_2, x_3) \\
B &amp;=&amp; (x_3, x_4) \\
A \cup B &amp; = &amp; (x_1, x_2, x_3, x_4) 
\end{array}\)</span>$ Oh yeah baby. Mathematics at its finest.</p>
<p>So, we’ve defined what we mean by <span class="math notranslate nohighlight">\(A \cap B\)</span> and <span class="math notranslate nohighlight">\(A \cup B\)</span>. Now let’s assign probabilities to these events. More specifically, let’s start by verifying the rule that claims that: $<span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span><span class="math notranslate nohighlight">\( Using our definitions earlier, we know that \)</span>A \cup B = (x_1, x_2, x_3, x_4)<span class="math notranslate nohighlight">\(, so \)</span><span class="math notranslate nohighlight">\(P(A \cup B) = P(x_1) + P(x_2) + P(x_3) + P(x_4)\)</span><span class="math notranslate nohighlight">\( and making similar use of the fact that we know what elementary events belong to \)</span>A<span class="math notranslate nohighlight">\(, \)</span>B<span class="math notranslate nohighlight">\( and \)</span>A \cap B<span class="math notranslate nohighlight">\(.... \)</span><span class="math notranslate nohighlight">\(\begin{array}{rcl}
P(A)  &amp;=&amp;   P(x_1) + P(x_2) + P(x_3)    \\
P(B) &amp;=&amp;  P(x_3) + P(x_4)  \\
P(A \cap B) &amp;=&amp;  P(x_3)
\end{array}\)</span><span class="math notranslate nohighlight">\( and therefore \)</span><span class="math notranslate nohighlight">\(\begin{array}{rcl}
P(A) + P(B) - P(A \cap B)
&amp;=&amp;  P(x_1) + P(x_2) + P(x_3) +  P(x_3) + P(x_4) -  P(x_3) \\
&amp;=&amp; P(x_1) + P(x_2) + P(x_3) + P(x_4) \\
&amp;=&amp; P(A \cup B)
\end{array}\)</span>$ Done.</p>
<p>The next concept we need to define is the notion of “<span class="math notranslate nohighlight">\(B\)</span> given <span class="math notranslate nohighlight">\(A\)</span>”, which is typically written <span class="math notranslate nohighlight">\(B | A\)</span>. Here’s what I mean: suppose that I get up one morning, and put on a pair of pants. An elementary event <span class="math notranslate nohighlight">\(x\)</span> has occurred. Suppose further I yell out to my wife (who is in the other room, and so cannot see my pants) “I’m wearing jeans today!”. Assuming that she believes that I’m telling the truth, she knows that <span class="math notranslate nohighlight">\(A\)</span> is true. <strong>Given</strong> that she knows that <span class="math notranslate nohighlight">\(A\)</span> has happened, what is the <strong>conditional probability</strong> that <span class="math notranslate nohighlight">\(B\)</span> is also true? Well, let’s think about what she knows. Here are the facts:</p>
<ul class="simple">
<li><p><strong>The non-jeans events are impossible</strong>. If <span class="math notranslate nohighlight">\(A\)</span> is true, then we know that the only possible elementary events that could have occurred are <span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(x_3\)</span> (i.e.,the jeans). The non-jeans events <span class="math notranslate nohighlight">\(x_4\)</span> and <span class="math notranslate nohighlight">\(x_5\)</span> are now impossible, and must be assigned probability zero. In other words, our <strong>sample space</strong> has been restricted to the jeans events. But it’s still the case that the probabilities of these these events <strong>must</strong> sum to 1: we know for sure that I’m wearing jeans.</p></li>
<li><p><strong>She’s learned nothing about which jeans I’m wearing</strong>. Before I made my announcement that I was wearing jeans, she already knew that I was five times as likely to be wearing blue jeans (<span class="math notranslate nohighlight">\(P(x_1) = 0.5\)</span>) than to be wearing black jeans (<span class="math notranslate nohighlight">\(P(x_3) = 0.1\)</span>). My announcement doesn’t change this… I said <strong>nothing</strong> about what colour my jeans were, so it must remain the case that <span class="math notranslate nohighlight">\(P(x_1) / P(x_3)\)</span> stays the same, at a value of 5.</p></li>
</ul>
<p>There’s only one way to satisfy these constraints: set the impossible events to have zero probability (i.e., <span class="math notranslate nohighlight">\(P(x | A) = 0\)</span> if <span class="math notranslate nohighlight">\(x\)</span> is not in <span class="math notranslate nohighlight">\(A\)</span>), and then divide the probabilities of all the others by <span class="math notranslate nohighlight">\(P(A)\)</span>. In this case, since <span class="math notranslate nohighlight">\(P(A) = 0.9\)</span>, we divide by 0.9. This gives:</p>
<p>|which pants?     |elementary event   |old prob, <span class="math notranslate nohighlight">\(P(x)\)</span>  | new prob, <span class="math notranslate nohighlight">\(P(x | A)\)</span>
|—————- |—————— |——————|———————-
|blue jeans       |<span class="math notranslate nohighlight">\(x_1\)</span>              |      0.5         |       0.556
|grey jeans       |<span class="math notranslate nohighlight">\(x_2\)</span>              |      0.3         |       0.333
|black jeans      |<span class="math notranslate nohighlight">\(x_3\)</span>              |      0.1         |       0.111
|black suit       |<span class="math notranslate nohighlight">\(x_4\)</span>              |       0          |         0
|blue tracksuit   |<span class="math notranslate nohighlight">\(x_5\)</span>              |      0.1         |         0</p>
<p>In mathematical terms, we say that $<span class="math notranslate nohighlight">\(P(x | A) = \frac{P(x)}{P(A)}\)</span><span class="math notranslate nohighlight">\( if \)</span>x \in A<span class="math notranslate nohighlight">\(, and \)</span>P(x|A) = 0<span class="math notranslate nohighlight">\( otherwise. And therefore... \)</span><span class="math notranslate nohighlight">\(\begin{array}{rcl}
P(B | A) &amp;=&amp; P(x_3 | A)  + P(x_4 | A) \\ \\
&amp;=&amp;  \displaystyle\frac{P(x_3)}{P(A)} + 0    \\ \\
&amp;=&amp; \displaystyle\frac{P(x_3)}{P(A)}
\end{array}\)</span><span class="math notranslate nohighlight">\( Now, recalling that \)</span>A \cap B = (x_3)<span class="math notranslate nohighlight">\(, we can write this as \)</span><span class="math notranslate nohighlight">\(P(B | A) = \frac{P(A \cap B)}{P(A)}\)</span><span class="math notranslate nohighlight">\( and if we multiply both sides by \)</span>P(A)<span class="math notranslate nohighlight">\( we obtain: \)</span><span class="math notranslate nohighlight">\(P(A \cap B) = P(B| A) P(A)\)</span>$ which is the third rule that we had listed in the table.</p>
</div>
</div>
<div class="section" id="the-binomial-distribution">
<h2>The binomial distribution<a class="headerlink" href="#the-binomial-distribution" title="Permalink to this headline">¶</a></h2>
<p>As you might imagine, probability distributions vary enormously, and there’s an enormous range of distributions out there. However, they aren’t all equally important. In fact, the vast majority of the content in this book relies on one of five distributions: the binomial distribution, the normal distribution, the <span class="math notranslate nohighlight">\(t\)</span> distribution, the <span class="math notranslate nohighlight">\(\chi^2\)</span> (“chi-square”) distribution and the <span class="math notranslate nohighlight">\(F\)</span> distribution. Given this, what I’ll do over the next few sections is provide a brief introduction to all five of these, paying special attention to the binomial and the normal. I’ll start with the binomial distribution, since it’s the simplest of the five.</p>
<div class="section" id="introducing-the-binomial">
<h3>Introducing the binomial<a class="headerlink" href="#introducing-the-binomial" title="Permalink to this headline">¶</a></h3>
<p>The theory of probability originated in the attempt to describe how games of chance work, so it seems fitting that our discussion of the <em>binomial distribution</em> should involve a discussion of rolling dice and flipping coins. Let’s imagine a simple “experiment”: in my hot little hand I’m holding 20 identical six-sided dice. On one face of each die there’s a picture of a skull; the other five faces are all blank. If I proceed to roll all 20 dice, what’s the probability that I’ll get exactly 4 skulls? Assuming that the dice are fair, we know that the chance of any one die coming up skulls is 1 in 6; to say this another way, the skull probability for a single die is approximately <span class="math notranslate nohighlight">\(.167\)</span>. This is enough information to answer our question, so let’s have a look at how it’s done.</p>
<p>As usual, we’ll want to introduce some names and some notation. We’ll let <span class="math notranslate nohighlight">\(N\)</span> denote the number of dice rolls in our experiment; which is often referred to as the <em>size parameter</em> of our binomial distribution. We’ll also use <span class="math notranslate nohighlight">\(\theta\)</span> to refer to the the probability that a single die comes up skulls, a quantity that is usually called the <em>success probability</em> of the binomial. Finally, we’ll use <span class="math notranslate nohighlight">\(X\)</span> to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of <span class="math notranslate nohighlight">\(X\)</span> is due to chance, we refer to it as a <em>random variable</em>. In any case, now that we have all this terminology and notation, we can use it to state the problem a little more precisely. The quantity that we want to calculate is the probability that <span class="math notranslate nohighlight">\(X = 4\)</span> given that we know that <span class="math notranslate nohighlight">\(\theta = .167\)</span> and <span class="math notranslate nohighlight">\(N=20\)</span>. The general “form” of the thing I’m interested in calculating could be written as $<span class="math notranslate nohighlight">\(P(X \ | \ \theta, N)\)</span><span class="math notranslate nohighlight">\( and we're interested in the special case where \)</span>X=4<span class="math notranslate nohighlight">\(, \)</span>\theta = .167<span class="math notranslate nohighlight">\( and \)</span>N=20<span class="math notranslate nohighlight">\(. There's only one more piece of notation I want to refer to before moving on to discuss the solution to the problem. If I want to say that \)</span>X<span class="math notranslate nohighlight">\( is generated randomly from a binomial distribution with parameters \)</span>\theta<span class="math notranslate nohighlight">\( and \)</span>N<span class="math notranslate nohighlight">\(, the notation I would use is as follows: \)</span><span class="math notranslate nohighlight">\(X \sim \mbox{Binomial}(\theta, N)\)</span>$</p>
<p>Yeah, yeah. I know what you’re thinking: notation, notation, notation. Really, who cares? Very few readers of this book are here for the notation, so I should probably move on and talk about how to use the binomial distribution. I’ve included the formula for the binomial distribution in Table [tab:distformulas], since some readers may want to play with it themselves, but since most people probably don’t care that much and because we don’t need the formula in this book, I won’t talk about it in any detail. Instead, I just want to show you what the binomial distribution looks like. To that end, Figure &#64;ref(fig:4binomial1) plots the binomial probabilities for all possible values of <span class="math notranslate nohighlight">\(X\)</span> for our dice rolling experiment, from <span class="math notranslate nohighlight">\(X=0\)</span> (no skulls) all the way up to <span class="math notranslate nohighlight">\(X=20\)</span> (all skulls). Note that this is basically a bar chart, and is no different to the “pants probability” plot I drew in Figure &#64;ref(fig:4pantsprob). On the horizontal axis we have all the possible events, and on the vertical axis we can read off the probability of each of those events. So, the probability of rolling 4 skulls out of 20 times is about 0.20 (the actual answer is 0.2022036, as we’ll see in a moment). In other words, you’d expect that to happen about 20% of the times you repeated this experiment.</p>
<img src="../../images/navarro_img/probability/binomSkulls20-eps-converted-to.png" alt="skulls" width="550"/>
<center>
The binomial distribution with size parameter of N =20 and an underlying success probability of 1/6. Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of X). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well.
</center></div>
<div class="section" id="working-with-the-binomial-distribution-in-python">
<h3>Working with the binomial distribution in Python<a class="headerlink" href="#working-with-the-binomial-distribution-in-python" title="Permalink to this headline">¶</a></h3>
<p>Python has a function called <code class="docutils literal notranslate"><span class="pre">binom.pdf</span></code> that calculates binomial probabilities for us. The main arguments to the function are</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">k</span></code> This is a number, or vector of numbers, specifying the outcomes whose probability you’re trying to calculate.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n</span></code> This is a number telling python the size of the experiment.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">p</span></code> This is the success probability for any one trial in the experiment.</p></li>
</ul>
<p>So, in order to calculate the probability of getting skulls, from an experiment of trials, in which the probability of getting a skull on any one trial is … well, the command I would use is simply this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.20220358121717216
</pre></div>
</div>
</div>
</div>
<p>To give you a feel for how the binomial distribution changes when we alter the values of <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(N\)</span>, let’s suppose that instead of rolling dice, I’m actually flipping coins. This time around, my experiment involves flipping a fair coin repeatedly, and the outcome that I’m interested in is the number of heads that I observe. In this scenario, the success probability is now <span class="math notranslate nohighlight">\(\theta = 1/2\)</span>. Suppose I were to flip the coin <span class="math notranslate nohighlight">\(N=20\)</span> times. In this example, I’ve changed the success probability, but kept the size of the experiment the same. What does this do to our binomial distribution?</p>
<img src="../../images/navarro_img/probability/Binomial2.png" alt="skulls" width="550"/>
<center>
Two binomial distributions, involving a scenario in which I'm flipping a fair coin, so the underlying success probability is 1/2. In panel (a), we assume I'm flipping the coin N = 20 times. In panel (b) we assume that the coin is flipped N = 100 times.</center><p>Well, as Figure &#64;ref(fig:4binomial2)a shows, the main effect of this is to shift the whole distribution, as you’d expect. Okay, what if we flipped a coin <span class="math notranslate nohighlight">\(N=100\)</span> times? Well, in that case, we get Figure &#64;ref(fig:4binomial2)b. The distribution stays roughly in the middle, but there’s a bit more variability in the possible outcomes.</p>
</div>
</div>
<div class="section" id="the-normal-distribution">
<h2>The normal distribution<a class="headerlink" href="#the-normal-distribution" title="Permalink to this headline">¶</a></h2>
<p>While the binomial distribution is conceptually the simplest distribution to understand, it’s not the most important one. That particular honour goes to the <em>normal distribution</em>, which is also referred to as “the bell curve” or a “Gaussian distribution”.</p>
<img src="../../images/navarro_img/probability/standardNormal-eps-converted-to.png" alt="normal" width="550"/>
<center>
The normal distribution with mean = 0 and standard deviation = 1. The x-axis corresponds to the value of some variable, and the y-axis tells us something about how likely we are to observe that value. However, notice that the y-axis is labelled Probability Density and not Probability. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the y axis behave a bit oddly: the height of the curve here isn't actually the probability of observing a particular x value. On the other hand, it is true that the heights of the curve tells you which x values are more likely (the higher ones!).
</center>
<p>A normal distribution is described using two parameters, the mean of the distribution <span class="math notranslate nohighlight">\(\mu\)</span> and the standard deviation of the distribution <span class="math notranslate nohighlight">\(\sigma\)</span>. The notation that we sometimes use to say that a variable <span class="math notranslate nohighlight">\(X\)</span> is normally distributed is as follows: $<span class="math notranslate nohighlight">\(X \sim \mbox{Normal}(\mu,\sigma)\)</span>$ Of course, that’s just notation. It doesn’t tell us anything interesting about the normal distribution itself. The mathematical formula for the normal distribution is:</p>
<img src="../../images/navarro_img/probability/Normal_formula.png" alt="normal" width="350"/>
<center>
Formula for the normal distribution
</center><p>The formula is important enough that everyone who learns statistics should at least look at it, but since this is an introductory text I don’t want to focus on it to much.</p>
<p>Instead of focusing on the maths, let’s try to get a sense for what it means for a variable to be normally distributed. To that end, have a look at Figure &#64;ref(fig:4normal), which plots a normal distribution with mean <span class="math notranslate nohighlight">\(\mu = 0\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma = 1\)</span>. You can see where the name “bell curve” comes from: it looks a bit like a bell. Notice that, unlike the plots that I drew to illustrate the binomial distribution, the picture of the normal distribution in Figure &#64;ref(fig:4normal) shows a smooth curve instead of “histogram-like” bars. This isn’t an arbitrary choice: the normal distribution is continuous, whereas the binomial is discrete. For instance, in the die rolling example from the last section, it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls.</p>
<p>With this in mind, let’s see if we can’t get an intuition for how the normal distribution works. Firstly, let’s have a look at what happens when we play around with the parameters of the distribution. One parameter we can change is the mean. This will shift the distribution to the right or left. The animation below shows a normal distribution with mean = 0, moving up and down from mean = 0 to mean = 5. Note, when you change the mean the whole shape of the distribution does not change, it just shifts from left to right. In the animation the normal distribution bounces up and down a little, but that’s just a quirk of the animation (plus it looks fund that way).</p>
<img src="../../images/gifs/normalMovingMean-1.gif" alt="skulls" width="550"/>
<center>A normal distribution with a moving mean.</center><p>In contrast, if we increase the standard deviation while keeping the mean constant, the peak of the distribution stays in the same place, but the distribution gets wider. The next animation shows what happens when you start with a small standard deviation (sd=0.5), and move to larger and larger standard deviation (up to sd =5). As you can see, the distribution spreads out and becomes wider as the standard deviation increases.</p>
<img src="../../images/gifs/normalMovingSD-1.gif" alt="skulls" width="550"/>
<center>A normal distribution with a moving standard deviation.</center><p>Notice, though, that when we widen the distribution, the height of the peak shrinks. This has to happen: in the same way that the heights of the bars that we used to draw a discrete binomial distribution have to <em>sum</em> to 1, the total <em>area under the curve</em> for the normal distribution must equal 1. Before moving on, I want to point out one important characteristic of the normal distribution. Irrespective of what the actual mean and standard deviation are, 68.3% of the area falls within 1 standard deviation of the mean. Similarly, 95.4% of the distribution falls within 2 standard deviations of the mean, and 99.7% of the distribution is within 3 standard deviations.</p>
<div class="section" id="probability-density">
<h3>Probability density<a class="headerlink" href="#probability-density" title="Permalink to this headline">¶</a></h3>
<p>There’s something I’ve been trying to hide throughout my discussion of the normal distribution, something that some introductory textbooks omit completely. They might be right to do so: this “thing” that I’m hiding is weird and counterintuitive even by the admittedly distorted standards that apply in statistics. Fortunately, it’s not something that you need to understand at a deep level in order to do basic statistics: rather, it’s something that starts to become important later on when you move beyond the basics. So, if it doesn’t make complete sense, don’t worry: try to make sure that you follow the gist of it.</p>
<p>Throughout my discussion of the normal distribution, there’s been one or two things that don’t quite make sense. Perhaps you noticed that the <span class="math notranslate nohighlight">\(y\)</span>-axis in these figures is labelled “Probability Density” rather than density.</p>
<p>When we’re talking about continuous distributions, it’s not meaningful to talk about the probability of a specific value. However, what we <em>can</em> talk about is the <strong>probability that the value lies within a particular range of values</strong>. To find out the probability associated with a particular range, what you need to do is calculate the “area under the curve”.</p>
<p>We won’t right now go through all of this but I will leave some example code here for playing with normal distributions in python.  For full documentation <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html">click here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="c1"># plots the shape of a normal distribution  (you can change these!)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">variance</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">variance</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">variance</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x11be2c748&gt;]
</pre></div>
</div>
</div>
</div>
<p>To compute the area under the curve between -1 and 0 you can use the “cumulative density function” which is basically the value of this integral up to a given value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">variance</span><span class="p">)</span> <span class="c1"># this &quot;freezes&quot; the distrubition with the given parameters saving some typing</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The value of the integral up to the value 0.0 is </span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The value of the integral up to the value -1.0 is </span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The value of the integral between -1.0  and 0.0 is </span><span class="si">{</span><span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-</span><span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The value of the integral up to the value 0.0 is 0.5
The value of the integral up to the value -1.0 is 0.15865525393145707
The value of the integral between -1.0  and 0.0 is 0.3413447460685429
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="other-useful-distributions">
<h2>Other useful distributions<a class="headerlink" href="#other-useful-distributions" title="Permalink to this headline">¶</a></h2>
<p>There are many other useful distributions, these include the <code class="docutils literal notranslate"><span class="pre">t</span></code> distribution, the <code class="docutils literal notranslate"><span class="pre">F</span></code> distribution, and the chi squared distribution. We will soon discover more about the <code class="docutils literal notranslate"><span class="pre">t</span></code> and <code class="docutils literal notranslate"><span class="pre">F</span></code> distributions when we discuss t-tests and ANOVAs in later chapters.</p>
</div>
<div class="section" id="summary-of-probability">
<h2>Summary of Probability<a class="headerlink" href="#summary-of-probability" title="Permalink to this headline">¶</a></h2>
<p>We’ve talked what probability means, and why statisticians can’t agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution, and spent a good chunk talking about some of the more important probability distributions that statisticians work with. We talked about things like this:</p>
<ul class="simple">
<li><p>Probability theory versus statistics</p></li>
<li><p>Frequentist versus Bayesian views of probability</p></li>
<li><p>Basics of probability theory</p></li>
<li><p>Binomial distribution, normal distribution</p></li>
</ul>
<p>As you’d expect, this coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the “simpler” task of documenting standard probability distributions is a big topic.Fortunately for you, very little of this is necessary. You’re unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won’t need them for this book, but it never hurts to know that there’s other possibilities out there.</p>
<p>Picking up on that last point, there’s a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often “forget” to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it’s important to understand these things before moving onto the applications. For example, there are a lot of rules about what you’re “allowed” to say when doing statistical inference, and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian/frequentist distinction.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/08"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Todd Gureckis<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>